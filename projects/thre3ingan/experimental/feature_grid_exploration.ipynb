{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import thre3d_atom.modules.thre3d_singan.volumetric_model as volume_model\n",
    "from thre3d_atom.utils.misc import batchify\n",
    "from thre3d_atom.rendering.volumetric.voxels import FeatureGrid, render_feature_grid, get_voxel_size_from_scene_bounds_and_hem_rad\n",
    "from thre3d_atom.modules.thre3d_singan.utils import render_image_in_chunks\n",
    "from thre3d_atom.utils.imaging_utils import pose_spherical, scale_camera_intrinsics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================\n",
    "# tweakbale hyperparameters for the notebook\n",
    "# ==========================================================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = Path(\"/home/animesh/work/ucl/projects/direct_supervision_thre3d_singan/vol_mod/fg_mlp\" +\n",
    "             \"/localized_feats/fish/model_final.pth\")\n",
    "\n",
    "camera_pose = pose_spherical(45, -30, 15.0)\n",
    "\n",
    "# ==========================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "camera_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_mod, extra_info = volume_model.create_vol_mod_from_saved_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_bounds = extra_info[\"scene_bounds\"]\n",
    "camera_intrinsics = scale_camera_intrinsics(extra_info[\"camera_intrinsics\"], 0.5)\n",
    "hem_rad = extra_info[\"hemispherical_radius\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# process the volumetric model:\n",
    "vol_mod = volume_model.process_hybrid_rgba_volumetric_model(vol_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the feature grid only at feature grid locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_mod.feature_grid.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vol_mod.feature_grid.features\n",
    "x_dim, y_dim, z_dim, nc = features.shape\n",
    "flat_features = features.reshape(-1, nc)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fixed_view_dir=-torch.ones(1, 3, device=device).repeat(flat_features.shape[0], 1)\n",
    "    rgba_values=batchify(\n",
    "        processor_fn=vol_mod.render_mlp,\n",
    "        collate_fn=partial(torch.cat, dim=0),\n",
    "        chunk_size=512,\n",
    "        verbose=True,\n",
    "    ) (torch.cat([flat_features, fixed_view_dir], dim=-1))\n",
    "    rgb_values, a_values = rgba_values[..., :3], rgba_values[..., 3:]\n",
    "    rgb_values = vol_mod._colour_producer(rgb_values)\n",
    "    a_values = vol_mod._transmittance_behaviour(a_values, torch.ones_like(a_values))\n",
    "    rgba_values = torch.cat([rgb_values, a_values], dim=-1)\n",
    "    \n",
    "    rgba_grid = rgba_values.reshape(x_dim, y_dim, z_dim, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgba_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgba_feature_grid = FeatureGrid(\n",
    "    features=rgba_grid.permute(3, 0, 1, 2),\n",
    "    voxel_size=get_voxel_size_from_scene_bounds_and_hem_rad(hem_rad, 128, scene_bounds),\n",
    "    tunable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render the decoded rgba-feature-grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_output_rgba = render_image_in_chunks(\n",
    "    cam_intrinsics=camera_intrinsics,\n",
    "    camera_pose=camera_pose,\n",
    "    num_rays_chunk=vol_mod._render_params.num_rays_chunk,\n",
    "    num_samples_per_ray=vol_mod._render_params.num_samples_per_ray,\n",
    "    feature_grid=rgba_feature_grid,\n",
    "    scene_bounds=scene_bounds,\n",
    "    density_noise_std=0.0,\n",
    "    perturb_sampled_points=False,\n",
    "    raw2alpha=lambda x, y: torch.clip(x, 0.0, 1.0),\n",
    "    colour_producer=lambda x: torch.clip(x, 0.0, 1.0),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_output_fg = render_image_in_chunks(\n",
    "    cam_intrinsics=camera_intrinsics,\n",
    "    camera_pose=camera_pose,\n",
    "    num_rays_chunk=vol_mod._render_params.num_rays_chunk,\n",
    "    num_samples_per_ray=vol_mod._render_params.num_samples_per_ray,\n",
    "    feature_grid=vol_mod.feature_grid,\n",
    "    processor_mlp=vol_mod.render_mlp,\n",
    "    scene_bounds=scene_bounds,\n",
    "    density_noise_std=0.0,\n",
    "    perturb_sampled_points=False,\n",
    "    raw2alpha=vol_mod._transmittance_behaviour,\n",
    "    colour_producer=vol_mod._colour_producer,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature directly:\n",
    "colour = rendered_output_fg.colour\n",
    "fig = plt.figure()\n",
    "plt.title(\"feature-grid_render\")\n",
    "plt.imshow(colour.detach().cpu().numpy())\n",
    "plt.savefig(f\"/home/animesh/feature_grid_render.png\", dpi=1000, facecolor=fig.get_facecolor(), edgecolor=\"none\")\n",
    "plt.show()\n",
    "\n",
    "# plot the Decoded RGBA render:\n",
    "colour = rendered_output_rgba.colour\n",
    "fig = plt.figure()\n",
    "plt.title(\"decoded RGBA render\")\n",
    "plt.imshow(colour.detach().cpu().numpy())\n",
    "plt.savefig(f\"/home/animesh/feature_location_decoded_rgba_render.png\", dpi=1000, facecolor=fig.get_facecolor(), edgecolor=\"none\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the feature grid at twice the resolution than the feature-grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vol_mod.feature_grid.features\n",
    "x_dim, y_dim, z_dim, nc = features.shape\n",
    "\n",
    "interpolated_features = F.interpolate(features.permute(3, 0, 1, 2)[None, ...], scale_factor=2, \n",
    "                                      mode=\"trilinear\", align_corners=False)[0].permute(1, 2, 3, 0)\n",
    "\n",
    "flat_features = interpolated_features.reshape(-1, nc)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fixed_view_dir=-torch.ones(1, 3, device=device).repeat(flat_features.shape[0], 1)\n",
    "    rgba_values=batchify(\n",
    "        processor_fn=vol_mod.render_mlp,\n",
    "        collate_fn=partial(torch.cat, dim=0),\n",
    "        chunk_size=512,\n",
    "        verbose=True,\n",
    "    ) (torch.cat([flat_features, fixed_view_dir], dim=-1))\n",
    "    rgb_values, a_values = rgba_values[..., :3], rgba_values[..., 3:]\n",
    "    rgb_values = vol_mod._colour_producer(rgb_values)\n",
    "    a_values = vol_mod._transmittance_behaviour(a_values, torch.ones_like(a_values))\n",
    "    rgba_values = torch.cat([rgb_values, a_values], dim=-1)\n",
    "    \n",
    "    rgba_grid = rgba_values.reshape(2 * x_dim, 2 * y_dim, 2 * z_dim, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgba_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgba_feature_grid = FeatureGrid(\n",
    "    features=rgba_grid.permute(3, 0, 1, 2),\n",
    "    voxel_size=get_voxel_size_from_scene_bounds_and_hem_rad(hem_rad, 256, scene_bounds),\n",
    "    tunable=False,\n",
    ")\n",
    "\n",
    "rendered_output_rgba = render_image_in_chunks(\n",
    "    cam_intrinsics=camera_intrinsics,\n",
    "    camera_pose=camera_pose,\n",
    "    num_rays_chunk=vol_mod._render_params.num_rays_chunk,\n",
    "    num_samples_per_ray=vol_mod._render_params.num_samples_per_ray,\n",
    "    feature_grid=rgba_feature_grid,\n",
    "    scene_bounds=scene_bounds,\n",
    "    density_noise_std=0.0,\n",
    "    perturb_sampled_points=False,\n",
    "    raw2alpha=lambda x, y: torch.clip(x, 0.0, 1.0),\n",
    "    colour_producer=lambda x: torch.clip(x, 0.0, 1.0),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# plot the Decoded RGBA render:\n",
    "colour = rendered_output_rgba.colour\n",
    "fig = plt.figure()\n",
    "plt.title(\"decoded RGBA render\")\n",
    "plt.imshow(colour.detach().cpu().numpy())\n",
    "plt.savefig(f\"/home/animesh/2x_resolution_fg_decoded_rgba_render.png\", dpi=1000, facecolor=fig.get_facecolor(), edgecolor=\"none\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoding the feature-grid at different locations for viewing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linspace(-1, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = vol_mod.feature_grid.features\n",
    "x_dim, y_dim, z_dim, nc = features.shape\n",
    "x_size, y_size, z_size = (2 / (x_dim - 1)), (2 / (y_dim - 1)), (2 / (z_dim - 1))\n",
    "\n",
    "points = torch.stack(torch.meshgrid(\n",
    "                        torch.linspace(-1, 1 - x_size, x_dim - 1, device=device),\n",
    "                        torch.linspace(-1, 1 - x_size, y_dim - 1, device=device),\n",
    "                        torch.linspace(-1, 1 - x_size, z_dim - 1, device=device),\n",
    "                     ), \n",
    "                     dim=-1)\n",
    "\n",
    "jitter_offset = (\n",
    "    torch.rand(size=(1, 1, 1, 3), device=device)\n",
    "    * torch.tensor(\n",
    "        [x_size, y_size, z_size], dtype=torch.float32, device=device\n",
    "    )[None, None, None, :]\n",
    ")\n",
    "\n",
    "jittered_points = (points + jitter_offset)[None, ...]\n",
    "\n",
    "point_features = F.grid_sample(\n",
    "    features[None, ...].permute(0, 4, 3, 2, 1),\n",
    "    jittered_points,\n",
    "    align_corners=True,\n",
    ")\n",
    "\n",
    "flat_features = point_features[0].permute(1, 2, 3, 0).reshape(-1, nc)\n",
    "\n",
    "with torch.no_grad():\n",
    "    random_view_dir = torch.rand(1, 3, device=device)\n",
    "    random_view_dir /= random_view_dir.norm(dim=-1, keepdim=True)\n",
    "    random_view_dir[..., -1] = -torch.abs(random_view_dir[..., -1])\n",
    "    random_view_dir = random_view_dir.repeat(flat_features.shape[0], 1)\n",
    "    \n",
    "    rgba_values=batchify(\n",
    "        processor_fn=vol_mod.render_mlp,\n",
    "        collate_fn=partial(torch.cat, dim=0),\n",
    "        chunk_size=512,\n",
    "        verbose=True,\n",
    "    ) (torch.cat([flat_features, random_view_dir], dim=-1))\n",
    "    rgb_values, a_values = rgba_values[..., :3], rgba_values[..., 3:]\n",
    "    rgb_values = vol_mod._colour_producer(rgb_values)\n",
    "    a_values = vol_mod._transmittance_behaviour(a_values, torch.ones_like(a_values))\n",
    "    rgba_values = torch.cat([rgb_values, a_values], dim=-1)\n",
    "    \n",
    "    rgba_grid = rgba_values.reshape(x_dim - 1, y_dim - 1, z_dim - 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgba_feature_grid = FeatureGrid(\n",
    "    features=rgba_grid.permute(3, 0, 1, 2),\n",
    "    voxel_size=get_voxel_size_from_scene_bounds_and_hem_rad(hem_rad, 127, scene_bounds),\n",
    "    tunable=False,\n",
    ")\n",
    "\n",
    "rendered_output_rgba = render_image_in_chunks(\n",
    "    cam_intrinsics=camera_intrinsics,\n",
    "    camera_pose=camera_pose,\n",
    "    num_rays_chunk=vol_mod._render_params.num_rays_chunk,\n",
    "    num_samples_per_ray=vol_mod._render_params.num_samples_per_ray,\n",
    "    feature_grid=rgba_feature_grid,\n",
    "    scene_bounds=scene_bounds,\n",
    "    density_noise_std=0.0,\n",
    "    perturb_sampled_points=False,\n",
    "    raw2alpha=lambda x, y: torch.clip(x, 0.0, 1.0),\n",
    "    colour_producer=lambda x: torch.clip(x, 0.0, 1.0),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# plot the Decoded RGBA render:\n",
    "colour = rendered_output_rgba.colour\n",
    "fig = plt.figure()\n",
    "plt.title(\"decoded RGBA render\")\n",
    "plt.imshow(colour.detach().cpu().numpy())\n",
    "plt.savefig(f\"/home/animesh/4.png\", dpi=1000, facecolor=fig.get_facecolor(), edgecolor=\"none\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
